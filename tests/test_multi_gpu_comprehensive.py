#!/usr/bin/env python3
"""
å¤šGPUåŠŸèƒ½ç»¼åˆæµ‹è¯•å¥—ä»¶
ä¸“ä¸ºå¤šGPUç¯å¢ƒè®¾è®¡ï¼ŒéªŒè¯å¤šGPUå¹¶è¡Œå¤„ç†çš„æ­£ç¡®æ€§ã€æ€§èƒ½å’Œç¨³å®šæ€§

è¿è¡Œç¯å¢ƒè¦æ±‚ï¼š
- è‡³å°‘2ä¸ªGPU
- CUDAå¯ç”¨
- è¶³å¤Ÿçš„GPUå†…å­˜

è¿è¡Œæ–¹å¼ï¼š
    python tests/test_multi_gpu_comprehensive.py

æˆ–è€…æŒ‡å®šGPUï¼š
    CUDA_VISIBLE_DEVICES=0,1,2,3 python tests/test_multi_gpu_comprehensive.py
"""

import os
import sys
import time
import json
import numpy as np
import torch
from typing import List, Dict
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from image_serialize.multi_gpu_extractor import MultiGPUFeatureExtractor
from image_serialize.feature_extractor import DINOv3FeatureExtractor
from image_serialize import ImageSerializer


class Colors:
    """ç»ˆç«¯é¢œè‰²è¾“å‡º"""
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    BOLD = '\033[1m'
    END = '\033[0m'


def print_header(text: str):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\n{Colors.BOLD}{Colors.BLUE}{'=' * 60}{Colors.END}")
    print(f"{Colors.BOLD}{Colors.BLUE}{text}{Colors.END}")
    print(f"{Colors.BOLD}{Colors.BLUE}{'=' * 60}{Colors.END}")


def print_success(text: str):
    """æ‰“å°æˆåŠŸä¿¡æ¯"""
    print(f"{Colors.GREEN}âœ“ {text}{Colors.END}")


def print_error(text: str):
    """æ‰“å°é”™è¯¯ä¿¡æ¯"""
    print(f"{Colors.RED}âœ— {text}{Colors.END}")


def print_warning(text: str):
    """æ‰“å°è­¦å‘Šä¿¡æ¯"""
    print(f"{Colors.YELLOW}âš  {text}{Colors.END}")


def create_test_images(num_images: int = 20, size: tuple = (224, 224)) -> List[np.ndarray]:
    """åˆ›å»ºæµ‹è¯•å›¾ç‰‡"""
    images = []
    for i in range(num_images):
        # åˆ›å»ºä¸åŒé¢œè‰²çš„æµ‹è¯•å›¾ç‰‡
        color = np.random.randint(0, 255, size=3)
        image = np.full((*size, 3), color, dtype=np.uint8)
        # æ·»åŠ éšæœºå™ªå£°ä½¿å›¾ç‰‡æœ‰æ‰€åŒºåˆ«
        noise = np.random.randint(0, 50, size=(*size, 3))
        image = np.clip(image.astype(int) + noise, 0, 255).astype(np.uint8)
        images.append(image)
    return images


def check_gpu_requirements() -> bool:
    """æ£€æŸ¥GPUç¯å¢ƒæ˜¯å¦æ»¡è¶³è¦æ±‚"""
    print_header("GPUç¯å¢ƒæ£€æŸ¥")
    
    if not torch.cuda.is_available():
        print_error("CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿è¡Œå¤šGPUæµ‹è¯•")
        return False
    
    gpu_count = torch.cuda.device_count()
    print(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
    
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        print(f"  GPU {i}: {props.name}")
        print(f"    æ€»å†…å­˜: {props.total_memory / 1024**3:.2f}GB")
        print(f"    è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    
    if gpu_count < 2:
        print_warning("åªæœ‰1ä¸ªGPUï¼Œéƒ¨åˆ†å¤šGPUæµ‹è¯•å°†è¢«è·³è¿‡")
        print_warning("å»ºè®®åœ¨å¤šGPUæœºå™¨ä¸Šè¿è¡Œå®Œæ•´æµ‹è¯•")
        return False
    
    print_success(f"æ»¡è¶³å¤šGPUæµ‹è¯•è¦æ±‚ï¼ˆ{gpu_count}ä¸ªGPUï¼‰")
    return True


def test_multi_gpu_initialization():
    """æµ‹è¯•å¤šGPUæå–å™¨åˆå§‹åŒ–"""
    print_header("æµ‹è¯•1: å¤šGPUæå–å™¨åˆå§‹åŒ–")
    
    try:
        gpu_count = torch.cuda.device_count()
        
        # æµ‹è¯•è‡ªåŠ¨GPUæ£€æµ‹
        print("\n[1.1] è‡ªåŠ¨GPUæ£€æµ‹")
        extractor_auto = MultiGPUFeatureExtractor()
        gpu_info = extractor_auto.get_gpu_info()
        print(f"  æ£€æµ‹åˆ°GPUæ•°é‡: {gpu_info['num_gpus']}")
        print(f"  è®¾å¤‡åˆ—è¡¨: {gpu_info['devices']}")
        assert gpu_info['num_gpus'] == gpu_count, "GPUæ•°é‡ä¸åŒ¹é…"
        print_success("è‡ªåŠ¨GPUæ£€æµ‹æ­£å¸¸")
        
        # æµ‹è¯•æŒ‡å®šGPU
        if gpu_count >= 2:
            print(f"\n[1.2] æŒ‡å®šGPU [{', '.join(map(str, range(min(2, gpu_count))))}]")
            extractor_specified = MultiGPUFeatureExtractor(gpus=list(range(min(2, gpu_count))))
            gpu_info_spec = extractor_specified.get_gpu_info()
            print(f"  æŒ‡å®šGPUè®¾å¤‡: {gpu_info_spec['devices']}")
            assert len(gpu_info_spec['devices']) == min(2, gpu_count), "æŒ‡å®šGPUæ•°é‡ä¸åŒ¹é…"
            print_success("æŒ‡å®šGPUåˆå§‹åŒ–æ­£å¸¸")
        
        # æµ‹è¯•CPUæ¨¡å¼
        print("\n[1.3] CPUå›é€€æ¨¡å¼")
        extractor_cpu = MultiGPUFeatureExtractor(gpus=['cpu'])
        gpu_info_cpu = extractor_cpu.get_gpu_info()
        print(f"  CPUæ¨¡å¼è®¾å¤‡: {gpu_info_cpu['devices']}")
        assert gpu_info_cpu['devices'] == ['cpu'], "CPUæ¨¡å¼åˆå§‹åŒ–å¤±è´¥"
        print_success("CPUå›é€€æ¨¡å¼æ­£å¸¸")
        
        # éªŒè¯æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        print("\n[1.4] æ¨¡å‹è®¾å¤‡éªŒè¯")
        for device, model in extractor_auto.models.items():
            model_device = next(model.parameters()).device
            expected_device = torch.device(device)
            print(f"  æ¨¡å‹ {device}: {model_device}")
            assert model_device == expected_device, f"æ¨¡å‹ä¸åœ¨é¢„æœŸè®¾å¤‡ä¸Š: {model_device} != {expected_device}"
        print_success("æ¨¡å‹è®¾å¤‡æ­£ç¡®")
        
        return True
        
    except Exception as e:
        print_error(f"åˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_multi_gpu_consistency():
    """æµ‹è¯•å¤šGPUä¸å•GPUç»“æœä¸€è‡´æ€§"""
    print_header("æµ‹è¯•2: å¤šGPUä¸å•GPUç»“æœä¸€è‡´æ€§")
    
    try:
        gpu_count = torch.cuda.device_count()
        if gpu_count < 2:
            print_warning("è·³è¿‡æ­¤æµ‹è¯•ï¼ˆéœ€è¦è‡³å°‘2ä¸ªGPUï¼‰")
            return True
        
        # åˆ›å»ºæµ‹è¯•å›¾ç‰‡
        test_images = create_test_images(32)
        
        # å•GPUå¤„ç†ï¼ˆä½¿ç”¨GPU 0ï¼‰
        print("\n[2.1] å•GPUåŸºå‡†æµ‹è¯•")
        single_extractor = DINOv3FeatureExtractor(batch_size=32)
        start_time = time.time()
        single_features = single_extractor.extract_features_batch(test_images)
        single_time = time.time() - start_time
        print(f"  å•GPUæ—¶é—´: {single_time:.4f}s")
        print(f"  ç‰¹å¾å½¢çŠ¶: {single_features.shape}")
        
        # å¤šGPUå¤„ç†ï¼ˆä½¿ç”¨GPU 0, 1ï¼‰
        print("\n[2.2] å¤šGPUå¹¶è¡Œæµ‹è¯•")
        multi_extractor = MultiGPUFeatureExtractor(gpus=[0, 1], batch_per_gpu=16)
        start_time = time.time()
        multi_features = multi_extractor.extract_features_batch_multi_gpu(test_images)
        multi_time = time.time() - start_time
        print(f"  å¤šGPUæ—¶é—´: {multi_time:.4f}s")
        print(f"  ç‰¹å¾å½¢çŠ¶: {multi_features.shape}")
        
        # æ¯”è¾ƒç»“æœ
        print("\n[2.3] ç»“æœä¸€è‡´æ€§éªŒè¯")
        single_np = single_features.cpu().numpy()
        multi_np = multi_features.cpu().numpy()
        
        # è®¡ç®—å·®å¼‚
        diff = np.abs(single_np - multi_np)
        max_diff = np.max(diff)
        mean_diff = np.mean(diff)
        
        print(f"  æœ€å¤§å·®å¼‚: {max_diff:.8f}")
        print(f"  å¹³å‡å·®å¼‚: {mean_diff:.8f}")
        print(f"  æ€§èƒ½æå‡: {single_time/multi_time:.2f}x")
        
        # éªŒè¯ä¸€è‡´æ€§ï¼ˆå…è®¸ä¸€å®šçš„æµ®ç‚¹è¯¯å·®ï¼‰
        tolerance = 1e-5
        if max_diff < tolerance:
            print_success(f"å¤šGPUä¸å•GPUç»“æœä¸€è‡´ï¼ˆå·®å¼‚ < {tolerance}ï¼‰")
            return True
        else:
            print_error(f"ç»“æœä¸ä¸€è‡´ï¼Œå·®å¼‚è¶…è¿‡å®¹å¿åº¦ {tolerance}")
            return False
            
    except Exception as e:
        print_error(f"ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_batch_distribution():
    """æµ‹è¯•æ‰¹æ¬¡åˆ†é…ç®—æ³•"""
    print_header("æµ‹è¯•3: æ‰¹æ¬¡åˆ†é…ç®—æ³•")
    
    try:
        gpu_count = torch.cuda.device_count()
        num_gpus_to_test = min(2, gpu_count) if gpu_count >= 2 else 1
        
        print(f"\nä½¿ç”¨ {num_gpus_to_test} ä¸ªGPUè¿›è¡Œæµ‹è¯•")
        
        extractor = MultiGPUFeatureExtractor(gpus=list(range(num_gpus_to_test)))
        
        # æµ‹è¯•ä¸åŒæ•°é‡çš„å›¾ç‰‡åˆ†é…
        test_cases = [5, 12, 25, 50, 100]
        
        print("\n[3.1] ä¸åŒæ•°é‡å›¾ç‰‡çš„åˆ†é…")
        for num_images in test_cases:
            test_images = create_test_images(num_images)
            distribution = extractor._distribute_images_to_gpus(test_images)
            
            total_allocated = sum(len(batch) for batch in distribution.values())
            print(f"  {num_images}å¼ å›¾ç‰‡ -> {[(device, len(batch)) for device, batch in distribution.items()]}")
            
            if total_allocated != num_images:
                print_error(f"åˆ†é…ä¸åŒ¹é…: æœŸæœ›{num_images}, å®é™…{total_allocated}")
                return False
        
        print_success("æ‰¹æ¬¡åˆ†é…ç®—æ³•æ­£ç¡®")
        
        # æµ‹è¯•è´Ÿè½½å‡è¡¡
        print("\n[3.2] è´Ÿè½½å‡è¡¡éªŒè¯")
        test_images = create_test_images(64)
        distribution = extractor._distribute_images_to_gpus(test_images)
        
        batch_sizes = [len(batch) for batch in distribution.values()]
        max_batch = max(batch_sizes)
        min_batch = min(batch_sizes)
        imbalance = (max_batch - min_batch) / max_batch if max_batch > 0 else 0
        
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_sizes}")
        print(f"  ä¸å¹³è¡¡åº¦: {imbalance:.2%}")
        
        if imbalance < 0.3:  # å…è®¸30%çš„ä¸å¹³è¡¡
            print_success("è´Ÿè½½å‡è¡¡è‰¯å¥½")
        else:
            print_warning(f"è´Ÿè½½ä¸å¹³è¡¡åº¦è¾ƒé«˜: {imbalance:.2%}")
        
        return True
        
    except Exception as e:
        print_error(f"æ‰¹æ¬¡åˆ†é…æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_performance_scaling():
    """æµ‹è¯•æ€§èƒ½æ‰©å±•æ€§"""
    print_header("æµ‹è¯•4: æ€§èƒ½æ‰©å±•æ€§")
    
    try:
        gpu_count = torch.cuda.device_count()
        if gpu_count < 2:
            print_warning("è·³è¿‡æ­¤æµ‹è¯•ï¼ˆéœ€è¦è‡³å°‘2ä¸ªGPUï¼‰")
            return True
        
        test_images = create_test_images(64)
        
        # æµ‹è¯•ä¸åŒGPUæ•°é‡çš„æ€§èƒ½
        results = []
        
        for num_gpus in [1, 2, min(4, gpu_count)]:
            print(f"\n[4.{num_gpus}] æµ‹è¯• {num_gpus} ä¸ªGPU")
            
            extractor = MultiGPUFeatureExtractor(
                gpus=list(range(num_gpus)),
                batch_per_gpu=32 // num_gpus
            )
            
            # é¢„çƒ­
            _ = extractor.extract_features_batch_multi_gpu(test_images)
            torch.cuda.synchronize()
            
            # æµ‹è¯•
            times = []
            for _ in range(3):
                start = time.time()
                features = extractor.extract_features_batch_multi_gpu(test_images)
                torch.cuda.synchronize()
                end = time.time()
                times.append(end - start)
            
            avg_time = np.mean(times)
            throughput = len(test_images) / avg_time
            results.append((num_gpus, avg_time, throughput))
            
            print(f"  å¹³å‡æ—¶é—´: {avg_time:.4f}s")
            print(f"  ååé‡: {throughput:.2f} images/sec")
        
        # è®¡ç®—æ‰©å±•æ•ˆç‡
        print("\n[4.4] æ‰©å±•æ€§åˆ†æ")
        baseline_time = results[0][1]
        print(f"  åŸºå‡†ï¼ˆ1 GPUï¼‰: {baseline_time:.4f}s")
        
        for num_gpus, avg_time, throughput in results[1:]:
            speedup = baseline_time / avg_time
            efficiency = speedup / num_gpus * 100
            print(f"  {num_gpus} GPU: {avg_time:.4f}s, åŠ é€Ÿæ¯”: {speedup:.2f}x, æ•ˆç‡: {efficiency:.1f}%")
        
        print_success("æ€§èƒ½æ‰©å±•æ€§æµ‹è¯•å®Œæˆ")
        return True
        
    except Exception as e:
        print_error(f"æ€§èƒ½æ‰©å±•æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_image_serializer_integration():
    """æµ‹è¯•ImageSerializerå¤šGPUé›†æˆ"""
    print_header("æµ‹è¯•5: ImageSerializerå¤šGPUé›†æˆ")
    
    try:
        gpu_count = torch.cuda.device_count()
        
        # æµ‹è¯•å¤šGPUæ¨¡å¼
        print("\n[5.1] å¤šGPUæ¨¡å¼åˆå§‹åŒ–")
        serializer_multi = ImageSerializer(
            db_path="test_db_integration_multi",
            use_multi_gpu=True,
            gpus=list(range(min(2, gpu_count))) if gpu_count >= 2 else [0],
            batch_per_gpu=16
        )
        
        print(f"  å¤šGPUæ¨¡å¼: {serializer_multi.use_multi_gpu}")
        print(f"  æå–å™¨ç±»å‹: {type(serializer_multi.feature_extractor).__name__}")
        
        if serializer_multi.use_multi_gpu:
            gpu_info = serializer_multi.feature_extractor.get_gpu_info()
            print(f"  GPUæ•°é‡: {gpu_info['num_gpus']}")
            print(f"  è®¾å¤‡åˆ—è¡¨: {gpu_info['devices']}")
        
        assert serializer_multi.use_multi_gpu, "æœªå¯ç”¨å¤šGPUæ¨¡å¼"
        print_success("å¤šGPUæ¨¡å¼åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•å•GPUæ¨¡å¼
        print("\n[5.2] å•GPUæ¨¡å¼åˆå§‹åŒ–")
        serializer_single = ImageSerializer(
            db_path="test_db_integration_single",
            use_multi_gpu=False,
            batch_size=32
        )
        
        print(f"  å¤šGPUæ¨¡å¼: {serializer_single.use_multi_gpu}")
        print(f"  æå–å™¨ç±»å‹: {type(serializer_single.feature_extractor).__name__}")
        
        assert not serializer_single.use_multi_gpu, "é”™è¯¯åœ°å¯ç”¨äº†å¤šGPUæ¨¡å¼"
        print_success("å•GPUæ¨¡å¼åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æ‰¹é‡å¤§å°æ›´æ–°
        print("\n[5.3] æ‰¹é‡å¤§å°æ›´æ–°")
        serializer_multi.update_batch_size(24)
        print(f"  æ›´æ–°åæ‰¹é‡å¤§å°: {serializer_multi.batch_size}")
        assert serializer_multi.batch_size == 24, "æ‰¹é‡å¤§å°æ›´æ–°å¤±è´¥"
        print_success("æ‰¹é‡å¤§å°æ›´æ–°æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print_error(f"ImageSerializeré›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print_header("æµ‹è¯•6: é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ")
    
    try:
        # æµ‹è¯•æ— æ•ˆGPU
        print("\n[6.1] æ— æ•ˆGPUå¤„ç†")
        try:
            MultiGPUFeatureExtractor(gpus=[999])
            print_error("åº”è¯¥æ‹’ç»æ— æ•ˆGPU")
            return False
        except (RuntimeError, ValueError) as e:
            print(f"  æ­£ç¡®æ‹’ç»æ— æ•ˆGPU: {e}")
            print_success("æ— æ•ˆGPUå¤„ç†æ­£ç¡®")
        
        # æµ‹è¯•ç©ºè¾“å…¥
        print("\n[6.2] ç©ºè¾“å…¥å¤„ç†")
        extractor = MultiGPUFeatureExtractor()
        empty_result = extractor.extract_features_batch_multi_gpu([])
        if empty_result.numel() == 0:
            print_success("ç©ºè¾“å…¥å¤„ç†æ­£ç¡®")
        else:
            print_error("ç©ºè¾“å…¥å¤„ç†å¤±è´¥")
            return False
        
        # æµ‹è¯•è´Ÿæ•°æ‰¹é‡å¤§å°
        print("\n[6.3] è´Ÿæ•°æ‰¹é‡å¤§å°å¤„ç†")
        try:
            extractor.update_batch_size(-1)
            print_error("åº”è¯¥æ‹’ç»è´Ÿæ•°æ‰¹é‡å¤§å°")
            return False
        except ValueError:
            print_success("è´Ÿæ•°æ‰¹é‡å¤§å°å¤„ç†æ­£ç¡®")
        
        # æµ‹è¯•é›¶æ‰¹é‡å¤§å°
        print("\n[6.4] é›¶æ‰¹é‡å¤§å°å¤„ç†")
        try:
            extractor.update_batch_size(0)
            print_error("åº”è¯¥æ‹’ç»é›¶æ‰¹é‡å¤§å°")
            return False
        except ValueError:
            print_success("é›¶æ‰¹é‡å¤§å°å¤„ç†æ­£ç¡®")
        
        return True
        
    except Exception as e:
        print_error(f"é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_large_batch_processing():
    """æµ‹è¯•å¤§æ‰¹é‡å¤„ç†"""
    print_header("æµ‹è¯•7: å¤§æ‰¹é‡å¤„ç†ç¨³å®šæ€§")
    
    try:
        gpu_count = torch.cuda.device_count()
        num_gpus = min(2, gpu_count) if gpu_count >= 2 else 1
        
        print(f"\nä½¿ç”¨ {num_gpus} ä¸ªGPUå¤„ç†å¤§æ‰¹é‡æ•°æ®")
        
        extractor = MultiGPUFeatureExtractor(
            gpus=list(range(num_gpus)),
            batch_per_gpu=32
        )
        
        # æµ‹è¯•ä¸åŒå¤§å°çš„æ‰¹é‡
        batch_sizes = [64, 128, 256]
        
        for batch_size in batch_sizes:
            print(f"\n[7.{batch_sizes.index(batch_size)+1}] å¤„ç† {batch_size} å¼ å›¾ç‰‡")
            
            test_images = create_test_images(batch_size)
            
            # é¢„çƒ­
            _ = extractor.extract_features_batch_multi_gpu(test_images)
            torch.cuda.synchronize()
            
            # æµ‹è¯•
            start = time.time()
            features = extractor.extract_features_batch_multi_gpu(test_images)
            torch.cuda.synchronize()
            end = time.time()
            
            elapsed = end - start
            throughput = batch_size / elapsed
            
            print(f"  å¤„ç†æ—¶é—´: {elapsed:.4f}s")
            print(f"  ååé‡: {throughput:.2f} images/sec")
            print(f"  ç‰¹å¾å½¢çŠ¶: {features.shape}")
            
            assert features.shape[0] == batch_size, f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…: {features.shape[0]} != {batch_size}"
        
        print_success("å¤§æ‰¹é‡å¤„ç†ç¨³å®šæ€§æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print_error(f"å¤§æ‰¹é‡å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def generate_test_report(results: Dict[str, bool]):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print_header("æµ‹è¯•æŠ¥å‘Š")
    
    total_tests = len(results)
    passed_tests = sum(results.values())
    failed_tests = total_tests - passed_tests
    
    print(f"\næ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"{Colors.GREEN}é€šè¿‡: {passed_tests}{Colors.END}")
    print(f"{Colors.RED}å¤±è´¥: {failed_tests}{Colors.END}")
    print(f"é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%")
    
    print("\nè¯¦ç»†ç»“æœ:")
    for test_name, passed in results.items():
        status = f"{Colors.GREEN}âœ“ é€šè¿‡{Colors.END}" if passed else f"{Colors.RED}âœ— å¤±è´¥{Colors.END}"
        print(f"  {test_name}: {status}")
    
    if passed_tests == total_tests:
        print(f"\n{Colors.BOLD}{Colors.GREEN}ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¤šGPUåŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚{Colors.END}")
        return 0
    else:
        print(f"\n{Colors.BOLD}{Colors.RED}âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚{Colors.END}")
        return 1


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print(f"\n{Colors.BOLD}{Colors.BLUE}")
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘        å¤šGPUç‰¹å¾æå–å™¨ç»¼åˆæµ‹è¯•å¥—ä»¶                        â•‘")
    print("â•‘     Multi-GPU Feature Extractor Comprehensive Test         â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print(f"{Colors.END}")
    
    # æ£€æŸ¥GPUç¯å¢ƒ
    has_multi_gpu = check_gpu_requirements()
    
    # å®šä¹‰æµ‹è¯•
    tests = [
        ("å¤šGPUæå–å™¨åˆå§‹åŒ–", test_multi_gpu_initialization),
        ("å¤šGPUä¸å•GPUä¸€è‡´æ€§", test_multi_gpu_consistency),
        ("æ‰¹æ¬¡åˆ†é…ç®—æ³•", test_batch_distribution),
        ("æ€§èƒ½æ‰©å±•æ€§", test_performance_scaling),
        ("ImageSerializeré›†æˆ", test_image_serializer_integration),
        ("é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ", test_error_handling),
        ("å¤§æ‰¹é‡å¤„ç†ç¨³å®šæ€§", test_large_batch_processing),
    ]
    
    # è¿è¡Œæµ‹è¯•
    results = {}
    for test_name, test_func in tests:
        try:
            # è·³è¿‡éœ€è¦å¤šGPUçš„æµ‹è¯•ï¼ˆå¦‚æœåªæœ‰å•GPUï¼‰
            if not has_multi_gpu and test_name in ["å¤šGPUä¸å•GPUä¸€è‡´æ€§", "æ€§èƒ½æ‰©å±•æ€§"]:
                print_warning(f"è·³è¿‡ '{test_name}'ï¼ˆéœ€è¦å¤šGPUç¯å¢ƒï¼‰")
                results[test_name] = True
                continue
            
            passed = test_func()
            results[test_name] = passed
        except Exception as e:
            print_error(f"æµ‹è¯• '{test_name}' å‘ç”Ÿå¼‚å¸¸: {e}")
            results[test_name] = False
    
    # ç”ŸæˆæŠ¥å‘Š
    return generate_test_report(results)


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)