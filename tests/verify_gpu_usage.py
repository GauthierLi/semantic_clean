#!/usr/bin/env python3
"""
éªŒè¯å¤šGPUæå–å™¨æ˜¯å¦æ­£ç¡®ä½¿ç”¨GPUçš„ç®€å•æµ‹è¯•è„šæœ¬
"""

import os
import sys
import torch
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from image_serialize.multi_gpu_extractor import MultiGPUFeatureExtractor
from image_serialize.feature_extractor import DINOv3FeatureExtractor


def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}")


def verify_single_gpu_on_device():
    """éªŒè¯å•GPUæå–å™¨æ¨¡å‹åœ¨GPUä¸Š"""
    print_section("1. éªŒè¯å•GPUæå–å™¨ï¼ˆDINOv3FeatureExtractorï¼‰")
    
    extractor = DINOv3FeatureExtractor(batch_size=32)
    
    # æ£€æŸ¥æ¨¡å‹è®¾å¤‡
    model_device = next(extractor.model.parameters()).device
    print(f"âœ“ æ¨¡å‹è®¾å¤‡: {model_device}")
    
    # éªŒè¯æ‰€æœ‰å‚æ•°éƒ½åœ¨GPUä¸Š
    all_on_gpu = all(p.device == model_device for p in extractor.model.parameters())
    print(f"âœ“ æ‰€æœ‰å‚æ•°åœ¨GPUä¸Š: {all_on_gpu}")
    
    # æµ‹è¯•æ¨ç†
    test_image = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
    feature = extractor.extract_features(test_image)
    print(f"âœ“ æ¨ç†æˆåŠŸï¼Œç‰¹å¾å½¢çŠ¶: {feature.shape}")
    
    return model_device.type == 'cuda'


def verify_multi_gpu_on_devices():
    """éªŒè¯å¤šGPUæå–å™¨æ¨¡å‹åœ¨å¤šä¸ªGPUä¸Š"""
    print_section("2. éªŒè¯å¤šGPUæå–å™¨ï¼ˆMultiGPUFeatureExtractorï¼‰")
    
    gpu_count = torch.cuda.device_count()
    num_gpus = min(2, gpu_count) if gpu_count >= 2 else 1
    
    print(f"ä½¿ç”¨ {num_gpus} ä¸ªGPUè¿›è¡Œæµ‹è¯•")
    
    extractor = MultiGPUFeatureExtractor(gpus=list(range(num_gpus)))
    
    # æ£€æŸ¥æ¯ä¸ªæ¨¡å‹è®¾å¤‡
    print("\næ¨¡å‹è®¾å¤‡æ£€æŸ¥:")
    all_correct = True
    for device, model in extractor.models.items():
        model_device = next(model.parameters()).device
        expected_device = torch.device(device)
        
        is_correct = model_device == expected_device
        all_on_gpu = all(p.device == model_device for p in model.parameters())
        
        print(f"  {device}: æ¨¡å‹åœ¨ {model_device} {'âœ“' if is_correct else 'âœ—'}")
        print(f"        æ‰€æœ‰å‚æ•°åœ¨GPUä¸Š: {all_on_gpu} {'âœ“' if all_on_gpu else 'âœ—'}")
        
        all_correct = all_correct and is_correct and all_on_gpu
    
    # æµ‹è¯•æ¨ç†
    test_images = [np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8) for _ in range(10)]
    features = extractor.extract_features_batch_multi_gpu(test_images)
    print(f"\nâœ“ æ¨ç†æˆåŠŸï¼Œç‰¹å¾å½¢çŠ¶: {features.shape}")
    
    return all_correct


def verify_data_on_gpu():
    """éªŒè¯è¾“å…¥æ•°æ®ä¹Ÿç§»åŠ¨åˆ°GPU"""
    print_section("3. éªŒè¯è¾“å…¥æ•°æ®ç§»åŠ¨åˆ°GPU")
    
    extractor = MultiGPUFeatureExtractor(gpus=[0])
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_images = [np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8) for _ in range(5)]
    
    # æ‰‹åŠ¨æ£€æŸ¥æ•°æ®è½¬æ¢è¿‡ç¨‹
    device = 'cuda:0'
    model = extractor.models[device]
    
    print("\næ•°æ®è½¬æ¢è¿‡ç¨‹:")
    for i, img in enumerate(test_images[:2]):  # åªæ£€æŸ¥å‰ä¸¤å¼ 
        tensor = extractor.transform(img)
        tensor_device = tensor.to(device)
        
        print(f"  å›¾ç‰‡ {i}:")
        print(f"    åŸå§‹æ•°æ®: {img.shape}, dtype={img.dtype}")
        print(f"    Transformå: {tensor.shape}, dtype={tensor.dtype}, device={tensor.device}")
        print(f"    ç§»åŠ¨åˆ°GPUå: {tensor_device.shape}, dtype={tensor_device.dtype}, device={tensor_device.device}")
    
    print("\nâœ“ è¾“å…¥æ•°æ®æ­£ç¡®ç§»åŠ¨åˆ°GPU")
    
    return True


def verify_gpu_memory_usage():
    """éªŒè¯GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print_section("4. éªŒè¯GPUå†…å­˜ä½¿ç”¨")
    
    if not torch.cuda.is_available():
        print("âš  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUå†…å­˜æµ‹è¯•")
        return True
    
    # æ¸…ç©ºç¼“å­˜
    torch.cuda.empty_cache()
    
    # åˆ›å»ºæå–å™¨
    extractor = MultiGPUFeatureExtractor(gpus=[0])
    
    # æ£€æŸ¥å†…å­˜ä½¿ç”¨
    device_id = 0
    allocated = torch.cuda.memory_allocated(device_id)
    reserved = torch.cuda.memory_reserved(device_id)
    
    print(f"\nGPU {device_id} å†…å­˜ä½¿ç”¨:")
    print(f"  å·²åˆ†é…: {allocated / 1024**3:.2f} GB")
    print(f"  å·²ä¿ç•™: {reserved / 1024**3:.2f} GB")
    
    # è¿è¡Œæ¨ç†
    test_images = [np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8) for _ in range(10)]
    features = extractor.extract_features_batch_multi_gpu(test_images)
    
    # æ£€æŸ¥æ¨ç†åçš„å†…å­˜ä½¿ç”¨
    allocated_after = torch.cuda.memory_allocated(device_id)
    reserved_after = torch.cuda.memory_reserved(device_id)
    
    print(f"\næ¨ç†åGPU {device_id} å†…å­˜ä½¿ç”¨:")
    print(f"  å·²åˆ†é…: {allocated_after / 1024**3:.2f} GB")
    print(f"  å·²ä¿ç•™: {reserved_after / 1024**3:.2f} GB")
    print(f"  åˆ†é…å¢åŠ : {(allocated_after - allocated) / 1024**3:.2f} GB")
    
    print("\nâœ“ GPUå†…å­˜ä½¿ç”¨æ­£å¸¸")
    
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n" + "="*60)
    print("  å¤šGPU GPUä½¿ç”¨éªŒè¯æµ‹è¯•")
    print("  Multi-GPU GPU Usage Verification Test")
    print("="*60)
    
    # æ£€æŸ¥CUDA
    print(f"\nCUDAå¯ç”¨: {torch.cuda.is_available()}")
    print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
    
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"  GPU {i}: {props.name}")
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("å•GPUæå–å™¨GPUä½¿ç”¨", verify_single_gpu_on_device),
        ("å¤šGPUæå–å™¨GPUä½¿ç”¨", verify_multi_gpu_on_devices),
        ("è¾“å…¥æ•°æ®GPUä½¿ç”¨", verify_data_on_gpu),
        ("GPUå†…å­˜ä½¿ç”¨", verify_gpu_memory_usage),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            passed = test_func()
            results.append((test_name, passed))
        except Exception as e:
            print(f"\nâœ— æµ‹è¯• '{test_name}' å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results.append((test_name, False))
    
    # æ€»ç»“
    print_section("æµ‹è¯•æ€»ç»“")
    passed = sum(1 for _, p in results if p)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        print(f"  {test_name}: {status}")
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å’Œæ•°æ®éƒ½æ­£ç¡®ä½¿ç”¨GPUã€‚")
        return 0
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥GPUé…ç½®ã€‚")
        return 1


if __name__ == "__main__":
    sys.exit(main())